{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dda037ad-49b6-4cc6-9374-781492bbd185",
      "metadata": {},
      "source": [
        "# Audio Preprocessing Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63497bce-b874-4fb0-bc7f-e66cb1a802e2",
      "metadata": {},
      "source": [
        "## Submitted by : S.Akil\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d3806c-887c-4279-bf7d-9e203b9bf532",
      "metadata": {},
      "source": [
        "#### Goal: Download public domain podcast-style audio ‚Üí 16kHz mono speech (basic cleaning, ready for transcription)\n",
        "\n",
        "#### Steps we will do:\n",
        "\n",
        "##### 1.Download audio files from online public domain sources (LibriVox MP3s).\n",
        "##### 2.Load with librosa (resamples automatically to 16kHz).\n",
        "##### 3.Convert stereo ‚Üí mono.\n",
        "##### 4.Normalize volume.\n",
        "##### 5.Trim leading/trailing silence (using librosa.effects.trim).\n",
        "##### 6.Basic energy-based silence removal (simple VAD alternative)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22eb5149-b082-4a04-8a10-8fc4c981dfa3",
      "metadata": {},
      "source": [
        "## 1. Install & Import (minimal ‚Äì run once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell1_imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa  # Audio loading, effects, and resampling wizard\n",
        "import numpy as np  # Math helper for arrays and peaks\n",
        "import soundfile as sf  # For saving the processed .wav (high-quality)\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "print(\"‚úÖ Imports loaded! Ready for step-by-step processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6613a49-93d9-467b-9e41-587017cf9e44",
      "metadata": {},
      "source": [
        "## Setup File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell2_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths once‚Äîuse these in all steps below\n",
        "sample_input = r'C:\\Users\\Akil S\\OneDrive\\Desktop\\infosys\\archive\\Data\\genres_original\\blues\\blues.00000.wav'\n",
        "sample_output = 'blues_processed_16kHz.wav'  # Saves in current Jupyter folder\n",
        "\n",
        "# Optional: Check if file exists\n",
        "import os\n",
        "if os.path.exists(sample_input):\n",
        "    print(f\"‚úÖ Input file ready: {sample_input}\")\n",
        "    print(f\"   File size: {os.path.getsize(sample_input) / (1024*1024):.1f} MB\")\n",
        "else:\n",
        "    print(\"‚ùå Input file not found‚Äîcheck path!\")\n",
        "\n",
        "print(\"üîÑ Setup complete. Run next cell to load audio.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b1a5b3-097a-4f77-a3d1-c9eca3c71118",
      "metadata": {},
      "source": [
        "## Step 1: Load the Original Audio (Raw Waveform + Sample Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell3_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load Original Audio (Raw Waveform + Sample Rate)\n",
        "# Why? Gets the raw data; keeps stereo if present for later averaging.\n",
        "\n",
        "if 'sample_input' not in locals():\n",
        "    print(\"‚ùå Run Cell 2 first to set paths!\")\n",
        "else:\n",
        "    try:\n",
        "        y_original, sr_original = librosa.load(sample_input, sr=None, mono=False)  # mono=False keeps stereo\n",
        "        print(f\"üîÑ Step 1: Loading {sample_input}\")\n",
        "        print(f\"   üìä Loaded: Duration={len(y_original)/sr_original:.2f}s, Channels={'Stereo' if y_original.ndim > 1 else 'Mono'}, SR={sr_original}Hz\")\n",
        "        print(f\"   üîç Shape: {y_original.shape} (samples)\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"‚ùå Load failed: {e}\")\n",
        "\n",
        "print(\"‚úÖ Step 1 complete. y_original and sr_original ready for next step.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37797379-a47d-4425-8a3d-f328a2618975",
      "metadata": {},
      "source": [
        "## Step 2: Resample to 16kHz & Convert to Mono"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell4_resample",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Resample to 16kHz & Convert to Mono\n",
        "# Why? 16kHz is efficient (cuts samples ~27% from 22kHz); mono averages L/R for simplicity.\n",
        "# Librosa resamples smoothly (anti-aliasing filters prevent distortion).\n",
        "\n",
        "if 'y_original' not in locals() or 'sr_original' not in locals():\n",
        "    print(\"‚ùå Run Cell 3 first to load audio!\")\n",
        "else:\n",
        "    target_sr = 16000  # Standard for speech/transcription\n",
        "    \n",
        "    # Resample first (works for mono or stereo)\n",
        "    y_resampled = librosa.resample(y_original, orig_sr=sr_original, target_sr=target_sr)\n",
        "    \n",
        "    # Convert to mono if stereo\n",
        "    if y_resampled.ndim > 1:  # Stereo shape: (2, samples)\n",
        "        y_mono = np.mean(y_resampled, axis=0)  # Average channels: (samples,)\n",
        "        print(f\"   üîÑ Averaged stereo channels to mono\")\n",
        "    else:\n",
        "        y_mono = y_resampled\n",
        "    \n",
        "    sr_processed = target_sr\n",
        "    print(f\"üîÑ Step 2: Resampling {sample_input}\")\n",
        "    print(f\"   üìä New: Duration={len(y_mono)/sr_processed:.2f}s, Mono, SR={sr_processed}Hz\")\n",
        "    print(f\"   üîç Shape: {y_mono.shape} (reduced from {y_original.shape})\")\n",
        "\n",
        "print(\"‚úÖ Step 2 complete. y_mono and sr_processed ready for next step.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535d2602-8589-4293-8fa7-eaee9a69d0c1",
      "metadata": {},
      "source": [
        "## Step 3: Trim Leading/Trailing Silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell5_trim",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Trim Silence (Remove Long Quiet Starts/Ends)\n",
        "# Why? GTZAN files often have fade-ins/outs‚Äîtrimming focuses on content, shortens clips.\n",
        "# top_db=20: Trim where amplitude < -20dB (quiet threshold‚Äîtune lower for more aggressive).\n",
        "\n",
        "if 'y_mono' not in locals() or 'sr_processed' not in locals():\n",
        "    print(\"‚ùå Run Cell 4 first to resample!\")\n",
        "else:\n",
        "    trim_db = 20  # dB threshold‚Äîhigher = less trimming\n",
        "    \n",
        "    y_trimmed, trim_info = librosa.effects.trim(y_mono, top_db=trim_db)\n",
        "    print(f\"üîÑ Step 3: Trimming silence in resampled audio\")\n",
        "    print(f\"   ‚úÇÔ∏è From {len(y_mono)/sr_processed:.2f}s to {len(y_trimmed)/sr_processed:.2f}s\")\n",
        "    print(f\"   üìç Trimmed indices: {trim_info} (start={trim_info[0]/sr_processed:.2f}s, end={trim_info[1]/sr_processed:.2f}s)\")\n",
        "\n",
        "print(\"‚úÖ Step 3 complete. y_trimmed ready for next step.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595e3b9d-7b65-4102-ad94-cff1a88cc5d7",
      "metadata": {},
      "source": [
        "## Step 4: Normalize Volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell6_normalize",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Normalize Amplitude (Scale to Peak [-1, 1])\n",
        "# Why? Ensures all tracks have similar volume‚Äîkey for fair feature extraction (e.g., MFCCs won't bias loud songs).\n",
        "# np.max(np.abs()) finds peak, divides to cap at 1 (preserves shape, just scales).\n",
        "\n",
        "if 'y_trimmed' not in locals():\n",
        "    print(\"‚ùå Run Cell 5 first to trim!\")\n",
        "else:\n",
        "    normalize = True  # Set False to skip\n",
        "    \n",
        "    if normalize:\n",
        "        peak = np.max(np.abs(y_trimmed))\n",
        "        if peak > 0:  # Avoid divide-by-zero (silent files)\n",
        "            y_normalized = y_trimmed / peak\n",
        "            print(f\"üîÑ Step 4: Normalizing trimmed audio\")\n",
        "            print(f\"   üìê Peak from {peak:.3f} to 1.0 (range now [{np.min(y_normalized):.3f}, {np.max(y_normalized):.3f}])\")\n",
        "        else:\n",
        "            y_normalized = y_trimmed  # Already silent\n",
        "            print(\"   ‚ö†Ô∏è File was silent‚Äîno normalization needed.\")\n",
        "    else:\n",
        "        y_normalized = y_trimmed\n",
        "        print(\"   ‚è≠Ô∏è Normalization skipped.\")\n",
        "\n",
        "print(\"‚úÖ Step 4 complete. y_normalized ready for next step.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1006ed9-e691-4750-9729-4ec029eb4a5c",
      "metadata": {},
      "source": [
        "## Step 5: Basic Energy-Based Silence Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell7_silence",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Reduce Silence (Gentle Compression for Background Noise)\n",
        "# Why? Attenuates quiet parts without full VAD‚Äîmakes speech pop, reduces noise.\n",
        "# Below -20dB? Scale down by 1/4 (e.g., -30dB ‚Üí -7.5dB output).\n",
        "\n",
        "if 'y_normalized' not in locals():\n",
        "    print(\"‚ùå Run Cell 6 first to normalize!\")\n",
        "else:\n",
        "    threshold_db = -20  # Attenuate below this\n",
        "    ratio = 4  # Compression: 4dB quiet input ‚Üí 1dB output\n",
        "    \n",
        "    # Convert to dB\n",
        "    y_db = librosa.amplitude_to_db(np.abs(y_normalized), ref=np.max)\n",
        "    \n",
        "    # Identify & attenuate quiet parts\n",
        "    below_threshold = y_db < threshold_db\n",
        "    gain = np.where(below_threshold, 1 / ratio, 1.0)  # 0.25x for silence, 1x for speech\n",
        "    y_compressed = y_normalized * (10 ** (gain / 20))  # dB to linear amplitude\n",
        "    \n",
        "    y_processed = y_compressed  # Final waveform\n",
        "    print(f\"üîÑ Step 5: Reducing silence in normalized audio\")\n",
        "    print(f\"   üîâ Applied {ratio}:1 compression below {threshold_db}dB\")\n",
        "    print(f\"   üìä Non-silent %: ~{100 * np.mean(np.abs(y_processed) > 0.01):.1f}% (rough activity estimate)\")\n",
        "\n",
        "print(\"‚úÖ Step 5 complete. y_processed ready for save/visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669d2536-c0ee-4d89-a34d-b62b2e5c0077",
      "metadata": {},
      "source": [
        "## Step 6: Save the Processed Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell8_save",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Save Processed Audio (High-Quality .wav)\n",
        "# Why? soundfile saves without compression loss‚Äîready for transcription or features.\n",
        "\n",
        "if 'y_processed' not in locals() or 'sr_processed' not in locals():\n",
        "    print(\"‚ùå Run Cell 7 first to process!\")\n",
        "else:\n",
        "    if 'sample_output' not in locals():\n",
        "        print(\"‚ùå Run Cell 2 first for output path!\")\n",
        "    else:\n",
        "        sf.write(sample_output, y_processed, sr_processed)\n",
        "        print(f\"üîÑ Step 6: Saving processed audio\")\n",
        "        print(f\"   üíæ Saved: {sample_output} (Duration={len(y_processed)/sr_processed:.2f}s, Mono 16kHz)\")\n",
        "        print(f\"   üîç Final shape: {y_processed.shape}\")\n",
        "\n",
        "print(\"‚úÖ All steps complete! Check your folder for the WAV file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1841cd4b-d51a-4e42-b6af-158aefec76ea",
      "metadata": {},
      "source": [
        "## Final Stats & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell9_stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Stats & Visualization\n",
        "# Why? Quantify changes; plot to \"see\" the waveform.\n",
        "\n",
        "if 'y_processed' not in locals() or 'sr_processed' not in locals():\n",
        "    print(\"‚ùå Run Cell 7 first to process!\")\n",
        "else:\n",
        "    print(f\"\\nüìà Final Stats:\")\n",
        "    print(f\" - Amplitude range: [{np.min(y_processed):.3f}, {np.max(y_processed):.3f}] (normalized!)\")\n",
        "    print(f\" - Zero-crossings (activity): ~{np.sum(np.diff(np.sign(y_processed)) != 0)/len(y_processed)*100:.1f}% non-silent\")\n",
        "    print(f\" - Total duration: {len(y_processed)/sr_processed:.2f}s\")\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    time_axis = np.linspace(0, len(y_processed) / sr_processed, len(y_processed))\n",
        "    plt.plot(time_axis, y_processed)\n",
        "    plt.title('Processed Audio: 16kHz Mono, Normalized, Silence Handled')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "print(\"üéâ Processing pipeline done! Experiment by re-running cells.\")\n"
      ]
    }
  ]
}

